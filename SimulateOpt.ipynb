{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install -q bayesian-optimization==1.2.0 #cma\n","!pip install -q cma\n","!pip install -q numpy==1.18.5\n","!pip install -q scikit-learn==0.22.2.post1 #0.23.0\n","!pip install -q scipy==1.4.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BG6fxnzZ_LP","executionInfo":{"status":"ok","timestamp":1712008085333,"user_tz":240,"elapsed":234085,"user":{"displayName":"Patrick Slade","userId":"07494709505261167105"}},"outputId":"dbbb880f-574d-450c-8e1c-3be40396baac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not build wheels for scikit-learn, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"]}]},{"cell_type":"code","metadata":{"id":"XeMeDuTOoEq6","outputId":"61448fc9-863e-4120-f290-b300fbf68cfe","executionInfo":{"status":"ok","timestamp":1712008132870,"user_tz":240,"elapsed":47542,"user":{"displayName":"Patrick Slade","userId":"07494709505261167105"}},"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form"},"source":["#@title Overview loading\n","\n","# Evaluating met and parameter data to understand metabolic landscape\n","# TODO: normalize by the 1,1 condition done every day? Or at least remove the offset based on that? use mass as well?\n","# TODO: for cost function, also penalize the regions with large std in predictions to prevent exploration of those, in case they are the wrong concavity?\n","\n","import os\n","from google.colab import drive\n","import numpy as np\n","print(np.version.version)\n","import matplotlib.pyplot as plt\n","import sklearn#==0.23.1\n","import scipy#==1.4.1\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","import pickle as pkl\n","from numpy.matlib import repmat\n","from bayes_opt import BayesianOptimization\n","import cma\n","from cma import CMAEvolutionStrategy\n","import random\n","\n","drive.mount('/content/drive/')\n","main_folder = 'drive/Shareddrives/Ability Lab/Simulating HILO/Code/'#\"drive/My Drive/WHILO/Met Landscape/Code/\"\n","save_dir = main_folder+\"opt_results/\"\n","\n","gp_dir = main_folder + 'save_GP/'\n","datasets = os.listdir(main_folder)\n","\n","if True: # hiding Gaussian re-loading details\n","    # re-loading GP kernels previously fit\n","    m_l = []\n","    data_folder = main_folder + 'save_GP/_data/'\n","    subj_inds = np.load(data_folder + 'subj_inds.npy')\n","    param_std = np.load(data_folder + 'param_std.npy')\n","    param_mean = np.load(data_folder + 'param_mean.npy')\n","    norm_params = np.load(data_folder + 'norm_params.npy')\n","    norm_met = np.load(data_folder + 'norm_met.npy')\n","    met_std = np.load(data_folder + 'met_std.npy')\n","    met_mean = np.load(data_folder + 'met_mean.npy')\n","\n","    num_subjects = 5\n","    gp_fold = main_folder + 'save_GP/' + '1_2 + WhiteKernel(noise_level=1) + Matern(length_scale=1, nu=0.5)/'\n","    for i in range(num_subjects):\n","        gp_l = pkl.load(open(gp_fold+'m'+str(i)+'.dump','rb'))\n","        m_l.append(gp_l)\n","\n","# could convert figs to tikz plots https://github.com/nschloe/tikzplotlib"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.25.2\n","Mounted at /content/drive/\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator GaussianProcessRegressor from version 0.22.2.post1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]}]},{"cell_type":"code","metadata":{"id":"U06frwRXXEy-"},"source":["#@title Function definitions\n","def optimize(method, dims, m_l, num_runs, converge_crit, meas_noise, param_range, x_mean, x_std, hyper_params, save_dir, save_name, param_bounds, seed=1, saving = False):\n","    # setting up GP process list\n","    if dims == 4:\n","        if time_varying:\n","            m_r_l = [[m_l[0],m_l[1]], [m_l[2],m_l[3]], [m_l[4],m_l[1]]]\n","        else:\n","            m_r_l = m_l\n","    elif dims == 8:\n","        m_r_l = [[m_l[0],m_l[1]], [m_l[2],m_l[3]]]\n","    elif dims == 12:\n","        m_r_l = [[m_l[0],m_l[1],m_l[2]], [m_l[2],m_l[3],m_l[4]]]\n","    elif dims == 16:\n","        m_r_l = [[m_l[0],m_l[1],m_l[2],m_l[3]]]\n","    elif dims == 20:\n","        m_r_l = [[m_l[0],m_l[1],m_l[2],m_l[3],m_l[4]]]\n","\n","    # running optimizations for each\n","    save_list = []\n","    global cnt\n","    for i,gp in enumerate(m_r_l):\n","        print(\"Model\",i)\n","        for j in range(num_runs):\n","            print(j)\n","            cnt = 0\n","            random.seed(seed)\n","            opt_inputs = [gp, converge_crit, meas_noise, param_range, x_mean, x_std, hyper_params, param_bounds]\n","            if method == 'ce':\n","                save_params = cross_entropy(opt_inputs)\n","            elif method == 'cma':\n","                save_params = cma(opt_inputs)\n","            elif method == 'bayes':\n","                save_params = bayes(opt_inputs)\n","            elif method == 'hyb':\n","                save_params = hybrid(opt_inputs)\n","            elif method == 'tbay':\n","                save_params = tuned_bayes(opt_inputs)\n","            else:\n","                print(\"Incorrect method assigned\")\n","            save_list.append(save_params)\n","    if saving:\n","        np.save(save_dir+save_name+'.npy', save_list)\n","    return save_list, len(m_r_l)\n","\n","def f(gp, x_eval, meas_noise=0., x_mean=0., x_std=1., std_scalar=5.3, min_dim = 4):\n","    # Returns function evaluation for a given GP model\n","    # Assumes normalized data and x_eval has shape [opt_dims,]\n","    if len(x_eval.shape) == 1:\n","        x_eval = np.expand_dims(x_eval,axis=-1)\n","    try:\n","        if time_varying:\n","            global cnt\n","            mean1 = gp[0].predict((x_eval.transpose()-x_mean)/x_std, return_std = False) # mean\n","            mean2 = gp[1].predict((x_eval.transpose()-x_mean)/x_std, return_std = False)\n","            count_frac = cnt/cnt_thresh\n","            mean = mean1*np.maximum(1-count_frac,0) + mean2*np.minimum(count_frac,1.)\n","            cnt += 1\n","        else:\n","            mean = 0\n","            dim_cnt = 0\n","            for gp_l in gp: # each one of these is a gp\n","                mean_temp = gp_l.predict((x_eval[dim_cnt:dim_cnt+min_dim].transpose()-x_mean[dim_cnt:dim_cnt+min_dim])/x_std[dim_cnt:dim_cnt+min_dim], return_std = False)\n","                mean += mean_temp\n","                dim_cnt+=min_dim\n","    except:\n","        mean = gp.predict((x_eval.transpose()-x_mean)/x_std, return_std = False)\n","\n","    return mean * (1.0 + random.gauss(0, meas_noise))#meas_noise*np.random.randn())\n","\n","def constrain_params(sampled_param, param_bounds):\n","    param_min = param_bounds[:,0]\n","    param_max = param_bounds[:,1]\n","    constrain_sample = np.minimum(np.maximum(sampled_param, param_min), param_max)\n","    return constrain_sample\n","\n","def hybrid(opt_inputs):\n","    mean_list = []\n","    global gp\n","    gp, converge_crit, meas_noise, param_range, x_mean, x_std, hyper_params, param_bounds = opt_inputs\n","    num_samples = hyper_params[4]\n","    num_hyb = hyper_params[5]\n","    N = param_bounds.shape[0]\n","    x = np.mean(param_bounds, axis=1)# init point randomly selected for mean\n","    init_pts = 0\n","    pbounds = {}\n","    probe_params = {} # sample same first point\n","    for i in range(N):\n","        key = 'p'+str(i)\n","        val = (param_bounds[i,0], param_bounds[i,1])\n","        pbounds[key] = val\n","        probe_params[key] = x[i]\n","    optimizer = BayesianOptimization(f=bayes_wrap, pbounds=pbounds, verbose=0)#, random_state=1)\n","    optimizer.probe(params=probe_params)\n","    optimizer.maximize(init_points=init_pts, n_iter=num_hyb-init_pts)\n","\n","    elite_dict = optimizer.max['params']\n","    print(optimizer.max['target'])\n","    cma_init = np.zeros(N)\n","    for i in range(N):\n","        key = 'p'+str(i)\n","        cma_init[i] = elite_dict[key]\n","\n","    # perform cma for rest of the steps\n","    hyper_params[7] = hyper_params[6] # set the cma sigma to hyb_sigma value\n","    hyper_params[8] = cma_init\n","    hyper_params[3] = int(np.ceil((num_samples - num_hyb)/(4+int(np.floor(3*np.log(N)))))) # changing the number of generations\n","    opt_inputs = [gp, converge_crit, meas_noise, param_range, x_mean, x_std, hyper_params, param_bounds]\n","    cma_save = cma(opt_inputs)\n","\n","    save_params = [optimizer.max['target'], optimizer.res, cma_save]\n","    return save_params\n","\n","def cma(opt_inputs):\n","    gp, converge_crit, meas_noise, param_range, x_mean, x_std, hyper_params, param_bounds = opt_inputs\n","    mean_list = []\n","    N = param_bounds.shape[0]\n","    cma_init = hyper_params[8] # cma_mean\n","    if np.sum(cma_init) == 0:\n","        x = np.mean(param_bounds, axis=1)# init point randomly selected for mean\n","    else:\n","        x = cma_init\n","    sigma = hyper_params[7]#4.0 #np.mean((np.amax(param_range,axis=1) - np.amin(param_range,axis=1))/4)\n","    lam = 4+int(np.floor(3*np.log(N)))\n","    stopeval = lam*(hyper_params[3]) # samples * generations\n","    xmean = x\n","    λ = 4+int(3*np.log(N))\n","    mu = λ // 2\n","    weights = np.log(mu + 1 / 2) - np.log(np.asarray(range(1, mu + 1))).astype(np.float32)\n","    weights = weights / np.sum(weights)\n","    mueff = (np.sum(weights) ** 2) / np.sum(weights ** 2)\n","\n","    cc = (4 + mueff / N) / (N + 4 + 2 * mueff / N)\n","    cs = (mueff + 2) / (N + mueff + 5)\n","    c1 = 2 / ((N + 1.3) ** 2 + mueff)\n","    cmu = min(1 - c1, 2 * (mueff - 2 + 1 / mueff) / ((N + 2) ** 2 + mueff))\n","    damps = 1 + 2 * max(0, ((mueff - 1) / (N + 1)) ** 0.5 - 1) + cs\n","\n","    pc = np.zeros(N).astype(np.float32)\n","    ps = np.zeros(N).astype(np.float32)\n","    B = np.eye(N, N).astype(np.float32)\n","    D = np.ones(N).astype(np.float32)\n","    C = B * np.diag(D ** 2) * B.T\n","    invsqrtC = B * np.diag(D ** -1) * B.T\n","    eigeneval = 0\n","    chiN = N ** 0.5 * (1 - 1 / (4 * N) + 1 / (21 * N ** 2))\n","\n","    local_cnt = 0\n","    while local_cnt < stopeval:\n","        arx = np.zeros((λ, N))\n","        arfitness = np.zeros(λ)\n","        for k in range(λ):\n","            if local_cnt == 0: # fist sample\n","                arx[k] = x\n","            else:\n","                arx[k] = constrain_params(xmean + sigma * B.dot(D * np.random.randn(N)), param_bounds)\n","            arfitness[k] = f(gp, arx[k], meas_noise, x_mean, x_std)\n","            local_cnt += 1\n","\n","        arindex = np.argsort(arfitness)\n","        arfit_old = np.copy(arfitness)\n","        arfitness = arfitness[arindex]\n","\n","        xold = xmean\n","        xmean = weights.dot(arx[arindex[0:mu]])\n","\n","        ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * invsqrtC.dot((xmean - xold) / sigma)\n","        hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - cs) ** (2 * local_cnt / λ)) / chiN < 1.4 + 2 / (N + 1)\n","        pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * ((xmean - xold) / sigma)\n","        artmp = (1 / sigma) * (arx[arindex[0:mu]] - xold)\n","        C = (1 - c1 - cmu) * C + c1 * (pc.dot(pc.T) + (1 - hsig) * cc * (2 - cc) * C) + cmu * artmp.T.dot(\n","            np.diag(weights)).dot(artmp)\n","        sigma = sigma * np.exp((cs / damps) * (np.linalg.norm(ps) / chiN - 1))\n","\n","        if local_cnt - eigeneval > λ / (c1 + cmu) / N / 10:\n","            eigeneval = local_cnt\n","            C = np.triu(C) + np.triu(C, 1).T\n","            D, B = np.linalg.eig(C)\n","            D = np.sqrt(D)\n","            invsqrtC = B.dot(np.diag(D ** -1).dot(B.T))\n","        mean_list.append((xmean, np.mean(arfitness[:mu]), sigma**2 * B * np.diag(D ** 2) * B.T, arfit_old))\n","    return mean_list\n","\n","def cross_entropy(opt_inputs):\n","    mean_list = []\n","    gp, converge_crit, meas_noise, param_range, x_mean, x_std, hyper_params, param_bounds = opt_inputs\n","    m = hyper_params[0]\n","    elite = hyper_params[1]\n","    num_gens = hyper_params[2]\n","    x_init = np.mean(param_bounds, axis=1)\n","    x = x_init\n","    cov = 16.0*np.eye(len(x_init))#(np.amax(param_range,axis=1) - np.amin(param_range,axis=1))/2.*np.eye(len(x_mean))\n","    not_done = True\n","    f_best = np.inf\n","    f_gen = np.zeros(m)\n","    gen_cnt = 0\n","    while(gen_cnt < num_gens):\n","        samples = np.random.multivariate_normal(x, cov, size=m)\n","        if gen_cnt == 0:\n","            samples[0,:] = x_init\n","        samples = constrain_params(samples, param_bounds)\n","        for i in range(m):\n","            f_gen[i] = f(gp, samples[i], meas_noise, x_mean, x_std)\n","        args = np.argsort(f_gen) # sort points by funct val, keep elite best\n","        elite_samples = args[:elite]\n","        if f_gen[elite_samples[0]] < f_best: # update best\n","            f_best = f_gen[elite_samples[0]]\n","            x_best = samples[elite_samples[0],:]\n","        x = np.mean(samples[elite_samples,:],axis=0)\n","        cov = np.cov(samples[elite_samples,:],rowvar=False)\n","        gen_cnt += 1\n","        mean_list.append((x, np.mean(f_gen[args[:elite]]), cov, np.copy(f_gen)))\n","    return mean_list\n","\n","def bayes_wrap(**kwargs):\n","    dim = len(kwargs)\n","    x_val = np.zeros(dim)\n","    for i,[key, value] in enumerate(kwargs.items()):\n","        x_val[i] = value\n","    f_val = -f(gp, x_val, meas_noise, x_mean, x_std)\n","    return f_val[0]\n","\n","def bayes(opt_inputs):\n","    mean_list = []\n","    global gp\n","    gp, converge_crit, meas_noise, param_range, x_mean, x_std, hyper_params, param_bounds = opt_inputs\n","    m = hyper_params[0]\n","    num_samples = hyper_params[4]\n","    N = param_bounds.shape[0]\n","    x = np.mean(param_bounds, axis=1)# init point randomly selected for mean\n","    init_pts = 0 #4+int(np.floor(3*np.log(N)))\n","    pbounds = {}\n","    probe_params = {} # sample same first point\n","    for i in range(N):\n","        key = 'p'+str(i)\n","        val = (param_bounds[i,0], param_bounds[i,1])\n","        pbounds[key] = val\n","        probe_params[key] = x[i]\n","    optimizer = BayesianOptimization(f=bayes_wrap, pbounds=pbounds, verbose=0)#, random_state=1)\n","    optimizer.probe(params=probe_params)\n","    #print(num_samples)\n","    optimizer.maximize(init_points=init_pts, n_iter=num_samples-init_pts)\n","    #print(len(optimizer.res))\n","    save_params = [optimizer.max['target'], optimizer.res,]\n","    return save_params\n","    # can prob estimate the confidence interval by doing:\n","    # def posterior(optimizer, x_obs, y_obs, grid):\n","    #     optimizer._gp.fit(x_obs, y_obs)\n","    #     print(dir(optimizer))\n","    #     mu, sigma = optimizer._gp.predict(grid, return_std=True)\n","    #     return mu, sigma\n","    # x_obs = np.array([[res[\"params\"][\"x\"]] for res in optimizer.res])\n","    # y_obs = np.array([res[\"target\"] for res in optimizer.res])\n","    # mu, sigma = posterior(optimizer, x_obs, y_obs, x) # x is either something like this: 'x': (2,4), or a range of values?\n","\n","def tuned_bayes(opt_inputs):\n","    mean_list = []\n","    global gp\n","    gp, converge_crit, meas_noise, param_range, x_mean, x_std, hyper_params, param_bounds = opt_inputs\n","    m = hyper_params[0]\n","    num_samples = hyper_params[4]\n","    N = param_bounds.shape[0]\n","    x = np.mean(param_bounds, axis=1)# init point randomly selected for mean\n","    init_pts = 8 #4+int(np.floor(3*np.log(N)))\n","    pbounds = {}\n","    probe_params = {} # sample same first point\n","    for i in range(N):\n","        key = 'p'+str(i)\n","        val = (param_bounds[i,0], param_bounds[i,1])\n","        pbounds[key] = val\n","        probe_params[key] = x[i]\n","    optimizer = BayesianOptimization(f=bayes_wrap, pbounds=pbounds, verbose=0)#, random_state=1)\n","    optimizer.probe(params=probe_params)\n","    optimizer.maximize(init_points=init_pts, n_iter=num_samples-init_pts, acq=\"ucb\", kappa_decay=0.93)\n","    save_params = [optimizer.max['target'], optimizer.res,]\n","    return save_params\n","\n","if True: # hiding plotting details\n","    # TODO: have separate plotting function/file that takes in list of results files to load, and corresponding labels and colors for plotting\n","    plt.rcParams['font.family'] = \"sans-serif\"\n","    plt.rcParams['font.sans-serif'] = 'Helvetica'\n","    plt.rcParams['axes.edgecolor']='k'\n","    plt.rcParams['axes.linewidth']=1.2\n","    plt.rcParams['xtick.color']='k'\n","    plt.rcParams['ytick.color']='k'\n","    plt_txt = 8\n","    def_marker_size=7\n","    plt.rcParams.update({'font.size': plt_txt})\n","    # color scheme\n","    black = 'k'\n","    grey = '#b7b6b6' #'#c7c6c6' #8f8e8d'\n","    dark_grey = '#838281'#737271'#646362'#'#414141'\n","    purple = '#f69cf4'\n","    lblue = '#5ac5ff'\n","    blue = '#0091ea'\n","    purple_as = '#bf32e7'\n","    green = '#00c853'\n","    orange = '#ffab00'\n","    red = '#ff3d00'\n","    alpha = 0.2\n","    def update_prop(handle, orig):\n","        handle.update_from(orig)\n","        handle.set_marker(\"s\")\n","        handle.set_markersize(def_marker_size)\n","    plt.rcParams['legend.handlelength'] = 0\n","    from matplotlib.legend_handler import HandlerLine2D\n","    import matplotlib.patches as mpatches\n","    from matplotlib.legend_handler import HandlerPatch\n","    class HandlerSquare(HandlerPatch):\n","        def create_artists(self, legend, orig_handle, xdescent, ydescent, width, height, fontsize, trans):\n","            center = xdescent + 0.5 * (width - height), ydescent\n","            p = mpatches.Rectangle(xy=center, width=height, height=height, angle=0.0)\n","            self.update_prop(p, orig_handle, legend)\n","            p.set_transform(trans)\n","            return [p]\n","\n","            # take the data pulled from the optimization and reformat it to plottable data\n","\n","def process_sim(method, save_list, num_runs, num_landscapes, dim, hyper_params):\n","    # output: x_mean_path [num_landscape, num_run, generations, dim], use large number placeholder for generations, store movement of mean val\n","        # sample_cnt  [num_landscape, num_run]\n","        # x_cov_path  [num_landscape, num_run, generations] # storing l2 norm of eig values\n","        # f_val       [num_landscape, num_run, generations] # storing average cost of elite samples\n","    if method == 'ce':\n","        samples_per_gen = hyper_params[0]\n","        num_gens = hyper_params[2]\n","        tot_samples = samples_per_gen*num_gens\n","    elif method == 'cma':\n","        num_gens = hyper_params[3]\n","        samples_per_gen = 4+int(np.floor(3*np.log(dim)))\n","        tot_samples = samples_per_gen*num_gens\n","    elif method == 'bayes' or method == 'tbay':\n","        tot_samples = hyper_params[4]\n","        num_gens = 1\n","        samples_per_gen = 1\n","    elif method == 'hyb':\n","        tot_samples = hyper_params[4]\n","        hyb_samples = hyper_params[5]\n","        samples_per_gen = 4+int(np.floor(3*np.log(dim)))\n","        num_gens = (tot_samples - hyb_samples)//samples_per_gen\n","    x_mean_path = np.zeros((num_landscapes, num_runs, num_gens, dim))\n","    x_cov_path = np.zeros((num_landscapes, num_runs, num_gens))\n","    f_val = np.zeros((num_landscapes, num_runs, num_gens))\n","    f_eval = np.zeros((num_landscapes, num_runs, tot_samples))\n","    f_eval_min = np.zeros((num_landscapes, num_runs, tot_samples))\n","    sample_cnt = np.zeros((num_landscapes, num_runs))\n","    for i in range(num_landscapes):\n","        for j in range(num_runs):\n","            l_i = j + i*num_runs\n","            # save list has [runs*landscapes] [generations] [values from individual run]\n","            eval_cnt = 0\n","            if method == 'cma' or method == 'ce':\n","                for k in range(num_gens):#len(save_list[l_i])): # num generations\n","                    x_mean_path[i,j,k,:] = save_list[l_i][k][0] # storing movement of mean values\n","                    f_val[i,j,k] = save_list[l_i][k][1] # storing cost values\n","                    cov = save_list[l_i][k][2] # storing movement of mean values\n","                    x_cov_path[i,j,k] = np.linalg.norm(np.linalg.eig(cov)[0])\n","                    f_eval[i,j,eval_cnt:eval_cnt+samples_per_gen] = save_list[l_i][k][3]\n","                    eval_cnt += samples_per_gen\n","            elif method == 'bayes' or method == 'tbay':\n","                best, param_list = save_list[l_i]\n","                #if tot_samples != len(param_list)+1:\n","                #    print(\"Error in number of samples for one trial:\", tot_samples, len(param_list))\n","                for k in range(tot_samples):\n","                    try:\n","                        sample = param_list[k]\n","                    except:\n","                        sample = param_list[-1]\n","                    f_eval[i,j,k] = -sample['target'] # can extract param w/ 'params'\n","            elif method == 'hyb':\n","                best, param_list, cma_list = save_list[l_i]\n","                for k in range(hyb_samples): # process bayes samples\n","                    try:\n","                        sample = param_list[k]\n","                    except:\n","                        sample = param_list[-1]\n","                    f_eval[i,j,k] = -sample['target'] # can extract param w/ 'params'\n","                eval_cnt = hyb_samples\n","                for k in range(num_gens):\n","                    f_eval[i,j,eval_cnt:eval_cnt+samples_per_gen] = cma_list[k][3]\n","                    eval_cnt += samples_per_gen\n","            for k in range(1,tot_samples): # re-arrange so always have minimum\n","                f_eval_min[i,j,k-1] = np.amin(f_eval[i,j,:k])\n","            ### check for convergence w/ sliding window here\n","    return x_mean_path, x_cov_path, f_val, sample_cnt, f_eval, f_eval_min\n","\n","def plot_perf(x_mean_path, x_cov_path, f_val, sample_cnt):\n","    fig, ax = plt.subplots(2, figsize=(10,14))\n","\n","    # plot change in cost as a function of the generations\n","    ax[0].plot(np.mean(f_val,axis=1).transpose())\n","    ax[0].set_ylabel(\"Function cost\")\n","    ax[0].set_xlabel(\"Generation\")\n","\n","    # plot change in sigma vs function of generations\n","    ax[1].plot(np.mean(x_cov_path,axis=1).transpose())\n","    ax[1].set_ylabel(\"Covariance matrix eigenvalue L2\")\n","    ax[1].set_xlabel(\"Generation\")\n","    plt.show()\n","\n","def plot_methods(methods, f_eval_list, f_eval_min_list, save_tag = \"None\", saving=False, m_c=[red,purple_as, blue, green]):\n","    fig, ax = plt.subplots(2, figsize=(3.4,7))\n","    for i, method in enumerate(methods):\n","        f_eval_min = f_eval_min_list[i] # [num subjs, num_runs, num_samples]\n","        f_eval_mean = np.mean(f_eval_min, axis=(0,1))[:min_total_samples-1]\n","        f_eval_std = np.mean(np.std(f_eval_min, axis=1),axis=0)[:min_total_samples-1]\n","        ax[0].plot(f_eval_mean, c=m_c[i], label=methods_labels[method])\n","        ax[0].fill_between(np.arange(len(f_eval_mean)),f_eval_mean + f_eval_std, f_eval_mean - f_eval_std, color=m_c[i], alpha = alpha)\n","        ax[0].set_title('Best sample so far')\n","        f_eval = np.mean(f_eval_list[i], axis=(0,1))[:min_total_samples]\n","        f_eval_spread = np.mean(np.std(f_eval_list[i], axis=1), axis=0)[:min_total_samples]\n","        f_eval_bottom = np.mean(np.amin(f_eval_list[i], axis=1), axis=0)[:min_total_samples]\n","        f_eval_top = np.mean(np.amax(f_eval_list[i], axis=1), axis=0)[:min_total_samples]\n","        #ax[1].fill_between(np.arange(len(f_eval)),f_eval + f_eval_spread, f_eval - f_eval_spread, color=m_c[i], alpha = alpha)\n","        ax[1].fill_between(np.arange(len(f_eval)),f_eval_bottom, f_eval_top, color=m_c[i], alpha = alpha)\n","        ax[1].set_title('Samples at each step')\n","        #ax[1].scatter(np.arange(len(f_eval)),f_eval, c=m_c[i])\n","    ax[0].set_ylabel(\"Metabolic cost\")\n","    ax[0].set_xlabel(\"Samples\")\n","    ax[0].spines['top'].set_color('none')\n","    ax[0].spines['right'].set_color('none')\n","    # plot funct value as samples (scatter)\n","    ax[1].set_ylabel(\"Metabolic cost\")\n","    ax[1].set_xlabel(\"Samples\")\n","    ax[1].spines['top'].set_color('none')\n","    ax[1].spines['right'].set_color('none')\n","    ax[0].legend(loc='upper right', frameon=False, handler_map={plt.Line2D:HandlerLine2D(update_func=update_prop)})\n","    plt.subplots_adjust(hspace=0.4)\n","\n","    if saving:\n","        save_fold = main_folder + 'plots/'\n","        filename = 'feval_plot' + save_tag\n","        plt.savefig(save_fold+filename+\".svg\", bbox_inches = 'tight', dpi=1200, pad_inches = 0)\n","        # plt.savefig(save_dir+filename+\".eps\", bbox_inches = 'tight', dpi=1200, pad_inches = 0)\n","        plt.savefig(save_fold+filename+\".png\", bbox_inches = 'tight', dpi=1200, pad_inches = 0)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gH-c1hA0XE1S","colab":{"base_uri":"https://localhost:8080/","height":478},"executionInfo":{"status":"error","timestamp":1712008516310,"user_tz":240,"elapsed":639,"user":{"displayName":"Patrick Slade","userId":"07494709505261167105"}},"outputId":"5465529c-9c5c-4f83-d1bb-b5a80b5360a0"},"source":["#@title Simulation function\n","\n","saving = True\n","dims = 4\n","methods = ['cma','ce'] #'tbay','bayes', # optimization algorithm\n","methods_labels = {'tbay':'Tuned Bayes','hyb':'Hybrid', 'ce':'Cross-Entropy', 'cma':'Covariance Matrix\\nAdaptation (ES)', 'bayes':'Bayesian\\nOptimization'}\n","meas_noise = 0.046 # measurement noise as a percent motivated by HILO paper\n","num_runs = 10 # number of random restarts for each method, for each GP model\n","num_gens_cma = 12 # 12 for short, 24 for long\n","num_gens_ce = int(np.maximum(1,num_gens_cma//2)) # error with size of CE when it gets larger than 2?\n","time_varying = False\n","cnt_thresh = 80. # number of samples to transition over\n","cnt = 0\n","hyb_init = 130\n","hyb_sigma = 0.3#0.3\n","sigma = 4.\n","if True: # hiding calculation of other parameters\n","    param_min = np.amin(norm_params,axis=0)\n","    param_max = np.amax(norm_params,axis=0)\n","    param_range = np.vstack((param_min,param_max)).transpose()\n","    param_bounds = np.zeros((4,2))\n","    param_bounds[0,:] = np.array([0,1.]) # max torque as % Nm/kg\n","    param_bounds[1,:] = np.array([35,55]) # limits on peak time bounds\n","    param_bounds[2,:] = np.array([10,40]) # rise/fall time bounds\n","    param_bounds[3,:] = np.array([5,20]) # rise/fall time bounds\n","    mult = dims//4\n","    norm_param_bounds = np.zeros((dims,2))\n","    for i in range(mult):\n","        norm_param_bounds[i*4:(i+1)*4,:] = (param_bounds - np.expand_dims(param_mean,axis=1))/np.expand_dims(param_std,axis=1)\n","    x_mean = np.zeros(dims)\n","    x_std = np.ones(dims)\n","    save_name = str(methods) + '_' + str(num_runs)\n","    converge_crit = 0.05# NOT USED RN\n","    cma_samples_per_gen = 4+int(np.floor(3*np.log(dims)))\n","    m = 2*cma_samples_per_gen\n","    elite = m//2\n","    min_total_samples = np.minimum(m*num_gens_ce, cma_samples_per_gen*num_gens_cma)\n","    cma_mean = np.zeros(dims)\n","    hyper_params = [m, elite, num_gens_ce, num_gens_cma, min_total_samples, hyb_init, hyb_sigma, sigma, cma_mean] # m, elite, num_gens (ce), num_gens (cma)\n","    seed = 1 # seed for rng\n","\n","# calling main optimize function\n","f_eval_list = []\n","f_eval_min_list = []\n","for i,method in enumerate(methods):\n","    print(method)\n","    np.random.seed(seed)\n","    save_list, num_landscapes = optimize(method, dims, m_l, num_runs,converge_crit, meas_noise, param_range, x_mean, x_std, hyper_params, save_dir, save_name, norm_param_bounds, seed)\n","    x_mean_path, x_cov_path, f_val, sample_cnt, f_eval, f_eval_min = process_sim(method, save_list, num_runs, num_landscapes, dims, hyper_params)\n","    f_eval_list.append(f_eval)\n","    f_eval_min_list.append(f_eval_min)\n","    if saving:\n","        np.save(save_dir+method+\"_feval_\"+str(num_runs)+\"_\"+str(len(x_mean))+\".npy\",f_eval)\n","        np.save(save_dir+method+\"_feval_min_\"+str(num_runs)+\"_\"+str(len(x_mean))+\".npy\",f_eval_min)\n","        if method == 'cma':\n","            np.save(save_dir+method+\"_mean_\"+str(num_runs)+\"_\"+str(len(x_mean))+\".npy\",x_mean_path)\n","            np.save(save_dir+method+\"_cov_\"+str(num_runs)+\"_\"+str(len(x_mean))+\".npy\",x_cov_path)\n","    #plot_perf(x_mean_path, x_cov_path, f_val, sample_cnt) # visualizing more details\n","plot_methods(methods, f_eval_list, f_eval_min_list)\n","# # TODO add plot overlay of contours and mean movement"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cma\n","Model 0\n","0\n","here\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'GaussianProcessRegressor' object has no attribute '_y_train_std'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-5d19b4da66e6>\u001b[0m in \u001b[0;36mf\u001b[0;34m(gp, x_eval, meas_noise, x_mean, x_std, std_scalar, min_dim)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mdim_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mgp_l\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# each one of these is a gp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim_cnt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdim_cnt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmin_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim_cnt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdim_cnt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmin_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx_std\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim_cnt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdim_cnt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmin_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'GaussianProcessRegressor' object is not iterable","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-7dd7bbcfefaf>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0msave_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_landscapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_runs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconverge_crit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeas_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_param_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mx_mean_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cov_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_cnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_eval_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_runs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_landscapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mf_eval_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-5d19b4da66e6>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(method, dims, m_l, num_runs, converge_crit, meas_noise, param_range, x_mean, x_std, hyper_params, save_dir, save_name, param_bounds, seed, saving)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0msave_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cma'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0msave_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bayes'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0msave_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-5d19b4da66e6>\u001b[0m in \u001b[0;36mcma\u001b[0;34m(opt_inputs)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0marx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstrain_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_bounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0marfitness\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeas_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mlocal_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-5d19b4da66e6>\u001b[0m in \u001b[0;36mf\u001b[0;34m(gp, x_eval, meas_noise, x_mean, x_std, std_scalar, min_dim)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mdim_cnt\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mmin_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgauss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeas_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#meas_noise*np.random.randn())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, return_std, return_cov)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;31m# undo normalisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0my_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_train_std\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_train_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;31m# if y_mean has shape (n_samples, 1), reshape to (n_samples,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'GaussianProcessRegressor' object has no attribute '_y_train_std'"]}]},{"cell_type":"code","metadata":{"id":"vPmoRhrwx5ie"},"source":["methods = methods = ['hyb','bayes','cma','ce']\n","methods_labels = {'ce':'Cross-Entropy', 'cma':'Covariance Matrix\\nAdaptation (ES)', 'bayes':'Bayesian\\nOptimization'}\n","saving = True\n","dim = dims\n","num_runs = num_runs\n","f_eval_list = []\n","f_eval_min_list = []\n","for method in methods:\n","    f_eval = np.load(save_dir+method+\"_feval_\"+str(num_runs)+\"_\"+str(dim)+\".npy\")\n","    f_eval_min = np.load(save_dir+method+\"_feval_min_\"+str(num_runs)+\"_\"+str(dim)+\".npy\")\n","    f_eval_list.append(f_eval)\n","    f_eval_min_list.append(f_eval_min)\n","plot_methods(methods, f_eval_list, f_eval_min_list, \"_\"+str(num_runs)+\"_\"+str(dim), saving)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S78_CxWXx0j6"},"source":["# make overlaid plotting for the version with and without the moving cost function over time\n","def plot_dual_methods(methods, f_eval_list, f_eval_min_list, f_eval_list2, f_eval_min_list2, save_tag = \"None\", saving=False, m_c=[purple_as, blue, green]):\n","    fig, ax = plt.subplots(2, figsize=(3.4,7))\n","    for i, method in enumerate(methods):\n","        f_eval_min = f_eval_min_list[i] # [num subjs, num_runs, num_samples]\n","        f_eval_mean = np.mean(f_eval_min, axis=(0,1))[:min_total_samples-1]\n","        f_eval_std = np.mean(np.std(f_eval_min, axis=1),axis=0)[:min_total_samples-1]\n","        ax[0].plot(f_eval_mean, c=m_c[i], label=methods_labels[method])\n","        ax[0].fill_between(np.arange(len(f_eval_mean)),f_eval_mean + f_eval_std, f_eval_mean - f_eval_std, color=m_c[i], alpha = alpha)\n","\n","        f_eval_mean2 = np.mean(f_eval_min2, axis=(0,1))[:min_total_samples-1]\n","\n","        f_eval = np.mean(f_eval_list[i], axis=(0,1))[:min_total_samples]\n","        f_eval_spread = np.mean(np.std(f_eval_list[i], axis=1), axis=0)[:min_total_samples]\n","        f_eval_bottom = np.mean(np.amin(f_eval_list[i], axis=1), axis=0)[:min_total_samples]\n","        f_eval_top = np.mean(np.amax(f_eval_list[i], axis=1), axis=0)[:min_total_samples]\n","        ax[0].plot(f_eval_mean2, c=m_c[i], linestyle='--')\n","        #ax[0].fill_between(np.arange(len(f_eval_mean)),f_eval_mean2 + f_eval_std2, f_eval_mean2 - f_eval_std2, color=m_c[i], alpha = alpha)\n","\n","        #ax[1].fill_between(np.arange(len(f_eval)),f_eval + f_eval_spread, f_eval - f_eval_spread, color=m_c[i], alpha = alpha)\n","        ax[1].fill_between(np.arange(len(f_eval)),f_eval_bottom, f_eval_top, color=m_c[i], alpha = alpha)\n","        ax[1].set_title('Samples at each step')\n","        #ax[1].scatter(np.arange(len(f_eval)),f_eval, c=m_c[i])\n","    ax[0].set_title('Best sample so far')\n","    ax[0].set_ylabel(\"Metabolic cost\")\n","    ax[0].set_xlabel(\"Samples\")\n","    ax[0].spines['top'].set_color('none')\n","    ax[0].spines['right'].set_color('none')\n","    # plot funct value as samples (scatter)\n","    # ax[1].set_ylabel(\"Function cost\")\n","    # ax[1].set_xlabel(\"Samples\")\n","    # ax[1].spines['top'].set_color('none')\n","    # ax[1].spines['right'].set_color('none')\n","    ax[0].legend(loc='upper right', frameon=False, handler_map={plt.Line2D:HandlerLine2D(update_func=update_prop)})\n","    plt.subplots_adjust(hspace=0.4)\n","\n","    if saving:\n","        save_fold = main_folder + 'plots/'\n","        filename = 'feval_plot' + save_tag\n","        plt.savefig(save_fold+filename+\".svg\", bbox_inches = 'tight', dpi=1200, pad_inches = 0)\n","        # plt.savefig(save_dir+filename+\".eps\", bbox_inches = 'tight', dpi=1200, pad_inches = 0)\n","        plt.savefig(save_fold+filename+\".png\", bbox_inches = 'tight', dpi=1200, pad_inches = 0)\n","    plt.show()\n","\n","methods = methods = ['bayes','cma','ce']\n","methods_labels = {'ce':'Cross-Entropy', 'cma':'Covariance Matrix\\nAdaptation (ES)', 'bayes':'Bayesian\\nOptimization'}\n","saving = True\n","dim = 4\n","num_runs = 10\n","num_runs2 = 19\n","f_eval_list = []\n","f_eval_min_list = []\n","for method in methods:\n","    f_eval = np.load(save_dir+method+\"_feval_\"+str(num_runs)+\"_\"+str(dim)+\".npy\")\n","    f_eval_min = np.load(save_dir+method+\"_feval_min_\"+str(num_runs)+\"_\"+str(dim)+\".npy\")\n","    f_eval_list.append(f_eval)\n","    f_eval_min_list.append(f_eval_min)\n","\n","f_eval_list2 = []\n","f_eval_min_list2 = []\n","for method in methods:\n","    f_eval2 = np.load(save_dir+method+\"_feval_\"+str(num_runs2)+\"_\"+str(dim)+\".npy\")\n","    f_eval_min2 = np.load(save_dir+method+\"_feval_min_\"+str(num_runs2)+\"_\"+str(dim)+\".npy\")\n","    f_eval_list2.append(f_eval2)\n","    f_eval_min_list2.append(f_eval_min2)\n","\n","plot_dual_methods(methods, f_eval_list, f_eval_min_list, f_eval_list2, f_eval_min_list2, \"_\"+str(num_runs)+\"_\"+str(dim)+'_comb', saving)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-a1tdYVNMvyi"},"source":["# make script to plot the performance across a range of dimensions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U-pXxkLKy094"},"source":["# scrap code testing other cma version\n","gp = m_l[0]\n","N = len(x_mean)\n","sigma = 1.0 # mult by something?\n","es = CMAEvolutionStrategy(N*[0], sigma)\n","iter_lim = 6\n","i = 0\n","while i <= iter_lim: #iter_limnot es.stop():\n","    solutions = es.ask()\n","    es.tell(solutions, [f(gp, x, meas_noise, x_mean, x_std) for x in solutions])\n","    es.logger.add()\n","    es.disp()\n","    i += 1\n","es.result_pretty()\n","es.logger.plot()"],"execution_count":null,"outputs":[]}]}